{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark\n",
    "conf = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# The directory for the file\n",
    "filename = \"q1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished. Return RDD\n",
    "def getData(sc, filename):\n",
    "    \"\"\"\n",
    "    Load data from raw text file into RDD and transform.\n",
    "    Hint: transfromation you will use: map(<lambda function>).\n",
    "    Args:\n",
    "        sc (SparkContext): spark context.\n",
    "        filename (string): hw2.txt cloud storage URI.\n",
    "    Returns:\n",
    "        RDD: RDD list of tuple of (<User>, [friend1, friend2, ... ]),\n",
    "        each user and a list of user's friends\n",
    "    \"\"\"\n",
    "    # read text file into RDD\n",
    "    data = sc.textFile(filename)\n",
    "\n",
    "    # TODO: implement your logic here\n",
    "    data = data.map(lambda line: np.array([str(x) for x in line.replace('\\n','').split('\\t')]))\n",
    "    data = data.map(lambda p:(int(p[0]), p[1].split(',')))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEdges(line):\n",
    "    # similar to mapFriends() in Q1, edges are direct friendship\n",
    "    friends = line[1]\n",
    "    user = line[0]\n",
    "\n",
    "    if friends != ['']:\n",
    "        for i in range(len(friends)):\n",
    "            # Direct friend\n",
    "            yield((user, int(friends[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Format data into edges and vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data in proper format\n",
    "data = getData(sc, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,), (1,), (2,), (3,), (4,)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get vertics\n",
    "vertices = data.map(lambda x: (x[0],))\n",
    "vertices.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get edges\n",
    "edges = data.flatMap(getEdges)\n",
    "edges.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert the RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Learning_Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert vertices to DF\n",
    "v = spark.createDataFrame(vertices,[\"id\"])\n",
    "v.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  0|  2|\n",
      "|  0|  3|\n",
      "|  0|  4|\n",
      "|  0|  5|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert edges to DF\n",
    "e = spark.createDataFrame(edges, [\"src\",\"dst\"])\n",
    "e.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('/Users/mac/Desktop/BigData/HW2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  0|  2|\n",
      "|  0|  3|\n",
      "|  0|  4|\n",
      "|  0|  5|\n",
      "|  0|  6|\n",
      "|  0|  7|\n",
      "|  0|  8|\n",
      "|  0|  9|\n",
      "|  0| 10|\n",
      "|  0| 11|\n",
      "|  0| 12|\n",
      "|  0| 13|\n",
      "|  0| 14|\n",
      "|  0| 15|\n",
      "|  0| 16|\n",
      "|  0| 17|\n",
      "|  0| 18|\n",
      "|  0| 19|\n",
      "|  0| 20|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = g.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49995"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1). Number of clusters in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "917"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(\"component\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = result.groupBy(\"component\").count().orderBy(\"count\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Top 10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|component|count|\n",
      "+---------+-----+\n",
      "|        0|48860|\n",
      "|    38403|   66|\n",
      "|    18466|   31|\n",
      "|    18233|   25|\n",
      "|    18891|   19|\n",
      "|      864|   16|\n",
      "|    49297|   13|\n",
      "|    19199|    6|\n",
      "|     7658|    5|\n",
      "|    22897|    4|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|     49045|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of users in the top 10 clusters\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "count.limit(10).agg(_sum(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49045"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm with another method\n",
    "from pyspark.sql.functions import col\n",
    "result.where(col(\"component\").isin({0,38403,18466,18233,18891,864,49297,19199,7658,22897})).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) List all 25 user IDS in cluster 18233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|component|\n",
      "+---------+\n",
      "|    18233|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count.filter(\"count=25\").select(\"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   id|\n",
      "+-----+\n",
      "|18233|\n",
      "|18234|\n",
      "|18235|\n",
      "|18236|\n",
      "|18237|\n",
      "|18238|\n",
      "|18239|\n",
      "|18240|\n",
      "|18241|\n",
      "|18242|\n",
      "|18243|\n",
      "|18244|\n",
      "|18245|\n",
      "|18246|\n",
      "|18247|\n",
      "|18248|\n",
      "|18249|\n",
      "|18250|\n",
      "|18251|\n",
      "|18252|\n",
      "|18253|\n",
      "|18254|\n",
      "|18255|\n",
      "|18256|\n",
      "|18257|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.filter(\"component=18233\").select(\"id\").show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Page rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4). Top 10 important users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = g.pageRank(tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|   id|          pagerank|\n",
      "+-----+------------------+\n",
      "|10164|17.315312963089895|\n",
      "|15496|14.866327204150846|\n",
      "|14689|12.685692559698428|\n",
      "|24966| 12.26882183906656|\n",
      "| 7884|11.827780808752543|\n",
      "|  934| 11.49589135687648|\n",
      "|45870| 11.27397140801791|\n",
      "| 5148|11.222433130678017|\n",
      "|20283| 11.14062997830236|\n",
      "|46039| 11.02696924843223|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr.vertices.select(\"id\", \"pagerank\").orderBy(\"pagerank\",ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5). Try different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
