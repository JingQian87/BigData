{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Columbia EECS E6893 Big Data Analytics\n",
    "\"\"\"\n",
    "This module is the spark streaming analysis process.\n",
    "\n",
    "\n",
    "Usage:\n",
    "    If used with dataproc:\n",
    "        gcloud dataproc jobs submit pyspark --cluster <Cluster Name> twitterHTTPClient.py\n",
    "\n",
    "    Create a dataset in BigQurey first using\n",
    "        bq mk bigdata_sparkStreaming\n",
    "\n",
    "    Remeber to replace the bucket with your own bucket name\n",
    "\n",
    "\n",
    "Todo:\n",
    "    1. hashtagCount: calculate accumulated hashtags count\n",
    "    2. wordCount: calculate word count every 60 seconds\n",
    "        the word you should track is listed below.\n",
    "    3. save the result to google BigQuery\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import subprocess\n",
    "import re\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# global variables\n",
    "bucket = \"bucket-hw3\"    # TODO : replace with your own bucket name\n",
    "output_directory_hashtags = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/hashtagsCount'.format(bucket)\n",
    "output_directory_wordcount = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/wordcount'.format(bucket)\n",
    "\n",
    "# output table and columns name\n",
    "output_dataset = ''                     #the name of your dataset in BigQuery\n",
    "output_table_hashtags = 'hashtags'\n",
    "columns_name_hashtags = ['hashtags', 'count']\n",
    "output_table_wordcount = 'wordcount'\n",
    "columns_name_wordcount = ['word', 'count', 'time']\n",
    "\n",
    "# parameter\n",
    "IP = 'localhost'    # ip port\n",
    "PORT = 9001       # port\n",
    "\n",
    "STREAMTIME = 600          # time that the streaming process runs\n",
    "\n",
    "WORD = ['data', 'spark', 'ai', 'movie', 'good']     #the words you should filter and do word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "    \n",
    "def hashtagCount(words):\n",
    "    \"\"\"\n",
    "    Calculate the accumulated hashtags count sum from the beginning of the stream\n",
    "    and sort it by descending order of the count.\n",
    "    Ignore case sensitivity when counting the hashtags:\n",
    "        \"#Ab\" and \"#ab\" is considered to be a same hashtag\n",
    "    You have to:\n",
    "    1. Filter out the word that is hashtags.\n",
    "       Hashtag usually start with \"#\" and followed by a series of alphanumeric\n",
    "    2. map (hashtag) to (hashtag, 1)\n",
    "    3. sum the count of current DStream state and previous state\n",
    "    4. transform unordered DStream to a ordered Dstream\n",
    "    Args:\n",
    "        dstream(DStream): stream of real time tweets\n",
    "    Returns:\n",
    "        DStream Object with inner structure (hashtag, count)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: insert your code here\n",
    "    import re\n",
    "    tagCounts = words.filter(lambda word: word.lower().startswith(\"#\") and re.match('^[a-zA-Z0-9]+$',word[1:])).\\\n",
    "                map(lambda word:(word.lower(),1))\n",
    "    tagCounts = tagCounts.updateStateByKey(aggregate_tags_count)\n",
    "    return tagCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCount(words):\n",
    "    \"\"\"\n",
    "    Calculte the count of 5 sepcial words in 60 seconds for every 60 seconds (window no overlap)\n",
    "    Your should:\n",
    "    1. filter the words, case insensitive.\n",
    "    2. count the word during a special window size\n",
    "    3. add a time related mark to the output of each window, ex: a datetime type\n",
    "    Hints:\n",
    "        You can take a look at reduceByKeyAndWindow transformation\n",
    "        Dstream is a series of rdd, each RDD in a DStream contains data from a certain interval\n",
    "        You may want to take a look of transform transformation of DStream when trying to add a time\n",
    "    Args:\n",
    "        dstream(DStream): stream of real time tweets\n",
    "    Returns:\n",
    "        DStream Object with inner structure (word, count, time)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: insert your code here\n",
    "    selected = words.filter(lambda x: any(word == x.lower() for word in WORD)).\\\n",
    "                map(lambda word:(word.lower(),1))\n",
    "    counts = selected.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 60, 60)\n",
    "    return counts.transform(lambda time, rdd: rdd.map(lambda x: (x,time)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: No output operations registered, so nothing to execute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.start.\n: java.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.streaming.DStreamGraph.validate(DStreamGraph.scala:168)\n\tat org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:513)\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:573)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3203d0505b0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# start streaming process, wait for 600s and then stop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTREAMTIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopGraceFully\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: No output operations registered, so nothing to execute'"
     ]
    }
   ],
   "source": [
    "# Spark settings\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local[2]')\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "# create spark context with the above configuration\n",
    "#sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create sql context, used for saving rdd\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "# create the Streaming Context from the above spark context with batch interval size 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"~/checkpoint_TwitterApp\")\n",
    "\n",
    "# read data from port 9001\n",
    "dataStream = ssc.socketTextStream(IP, PORT)\n",
    "#dataStream.pprint()\n",
    "\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# calculate the accumulated hashtags count sum from the beginning of the stream\n",
    "topTags = hashtagCount(words)\n",
    "# topTags.pprint()\n",
    "\n",
    "# Calculte the word count during each time period 60s\n",
    "wordCount = wordCount(words)\n",
    "#wordCount.pprint()\n",
    "\n",
    "# start streaming process, wait for 600s and then stop.\n",
    "ssc.start()\n",
    "time.sleep(STREAMTIME)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "# save hashtags count and word count to google storage\n",
    "# used to save to google BigQuery\n",
    "# You should:\n",
    "#   1. topTags: only save the lastest rdd in DStream\n",
    "#   2. wordCount: save each rdd in DStream\n",
    "# Hints:\n",
    "#   1. You can take a look at foreachRDD transformation\n",
    "#   2. You may want to use helper function saveToStorage\n",
    "#   3. You should use save output to output_directory_hashtags, output_directory_wordcount,\n",
    "#       and have output columns name columns_name_hashtags and columns_name_wordcount.\n",
    "# TODO: insert your code here\n",
    "\n",
    "def sendPartition(iter):\n",
    "    # ConnectionPool is a static, lazily initialized pool of connections\n",
    "    connection = ConnectionPool.getConnection()\n",
    "    for record in iter:\n",
    "        connection.send(record)\n",
    "    # return to the pool for future reuse\n",
    "    ConnectionPool.returnConnection(connection)\n",
    "\n",
    "rdd_tags = topTags.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n",
    "rdd_words = wordCount.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n",
    "\n",
    "saveToStorage(rdd, output_directory_hashtags, columns_name_hashtags, \"overwirte\")\n",
    "saveToStorage(rdd, output_directory_wordcount, columns_name_wordcount, \"append\")\n",
    "\n",
    "# put the temp result in google storage to google BigQuery\n",
    "saveToBigQuery(sc, output_dataset, output_table_hashtags, output_directory_hashtags)\n",
    "saveToBigQuery(sc, output_dataset, output_table_wordcount, output_directory_wordcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def saveToStorage(rdd, output_directory, columns_name, mode):\n",
    "    \"\"\"\n",
    "    Save each RDD in this DStream to google storage\n",
    "    Args:\n",
    "        rdd: input rdd\n",
    "        output_directory: output directory in google storage\n",
    "        columns_name: columns name of dataframe\n",
    "        mode: mode = \"overwirte\", overwirte the file\n",
    "              mode = \"append\", append data to the end of file\n",
    "    \"\"\"\n",
    "    if not rdd.isEmpty():\n",
    "        (rdd.toDF( columns_name ) \\\n",
    "        .write.save(output_directory, format=\"json\", mode=mode))\n",
    "\n",
    "\n",
    "def saveToBigQuery(sc, output_dataset, output_table, directory):\n",
    "    \"\"\"\n",
    "    Put temp streaming json files in google storage to google BigQuery\n",
    "    and clean the output files in google storage\n",
    "    \"\"\"\n",
    "    files = directory + '/part-*'\n",
    "    subprocess.check_call(\n",
    "        'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "        '--replace '\n",
    "        '--autodetect '\n",
    "        '{dataset}.{table} {files}'.format(\n",
    "            dataset=output_dataset, table=output_table, files=files\n",
    "        ).split())\n",
    "    output_path = sc._jvm.org.apache.hadoop.fs.Path(directory)\n",
    "    output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "        output_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "\n",
    "    # put the temp result in google storage to google BigQuery\n",
    "    # saveToBigQuery(sc, output_dataset, output_table_hashtags, output_directory_hashtags)\n",
    "    # saveToBigQuery(sc, output_dataset, output_table_wordcount, output_directory_wordcount)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
